import pandas as pd
import math
import os
import json
from youtube_transcript_api import YouTubeTranscriptApi

def search_youtube(youtube_api, q="children|kids|kid|teen|teens", pageToken=None, topicId=None, debug=False, order="relevance"):
    '''
    Take a query and an optional nextPageToken, and return the youtube.search().list() results parsed
    as a DataFrame.
    
    Returns: (nextPageToken, response)
    
    Quota cost: 100 units.
    '''
    request = youtube_api.search().list(
            part="snippet",
            q=q,
            pageToken=pageToken,
            safeSearch="strict",
            maxResults=50,
            videoCaption="closedCaption",
            type="video",
            regionCode="US",
            relevanceLanguage="en",
            topicId=topicId,
            order=order # date | rating | relevance | title | videoCount | viewCount
    )
    response = request.execute()
    if debug:
        return response
    df = search_results_to_df(response)
    try:
        return response['nextPageToken'], df
    except:
        print(response)
        return None, df

def parse_search_result(item):
    row = dict(videoId=item['id']['videoId'],
               title=item['snippet']['title'])
    
    for k in ['description', 'publishedAt', 'channelTitle', 'tags', 'categoryId']:
        if k in item['snippet']:
            row[k] = item['snippet'][k]
    return row

def search_results_to_df(response):
    parsed = []
    if 'items' not in response:
        return None
    for item in response['items']:
        try:
            parsed_item = parse_search_result(item)
            parsed.append(parsed_item)
        except:
            continue
    return pd.DataFrame(parsed)


def load_video_details(youtube_api, idlist, debug=False):
    '''
    Once ids for videos have been collected from search, you can ask for more details
    '''
    request = youtube_api.videos().list(
        part="status,topicDetails,contentDetails,id",
        id=",".join(idlist),
        maxResults=len(idlist)
    )
    response = request.execute()
    if debug:
        return response
    details = pd.DataFrame([parse_detail_item(item) for item in response['items']])
    return details

def augment_initial_search(youtube_api, df):
    details_collector = []
    for i in range(math.ceil(len(df)/50)):
        print(i, end=',')
        subset = df.iloc[i*50:i*50+50]
        details = load_video_details(youtube_api, subset.videoId.tolist())
        details_collector.append(details)
    all_details = pd.concat(details_collector)
    print()
    return df.merge(all_details, how='left', on='videoId')

def parse_detail_item(item):
    ''' Extract important details from results of a call to video.list() which included
        parts id,status,contentDetail,topicDetail
    '''
    details = dict(videoId=item['id'])

    if 'contentDetails' in item:
        for contentDetail in ['duration', 'caption']:
            if contentDetail in item['contentDetails']:
                details[contentDetail] = item['contentDetails'][contentDetail]

    if 'topicDetails' in item:
        for topicDetail in ['relevantTopicIds', 'topicCategories']:
            if topicDetail in item['topicDetails']:
                details[topicDetail] = item['topicDetails'][topicDetail]
            
    if 'status' in item:
        for status in ['madeForKids']:
            if status in item['status']:
                details[status] = item['status'][status]

    return details


def fetch_transcript(videoId):
    transcripts = list(YouTubeTranscriptApi.list_transcripts(videoId))
    transcripts = [t for t in transcripts if 'english' in t.language.lower()]
    
    if len(transcripts) > 0:
        try:
            # try for first non-autogenerated, if it exists
            transcript = [t for t in transcripts if not t.is_generated][0]
        except:
            transcript = transcripts[0]
        text = transcript.fetch()
        return text
    else:
        return None
    
def fetch_and_save_transcript(videoId, save_dir, check_exists=True):
    fpath = os.path.join(save_dir, videoId+'.json')
    if check_exists and os.path.exists(fpath):
        return None
    
    text = fetch_transcript(videoId)
    with open(fpath, mode='w') as f:
        json.dump(text, f)
        
        
class YTCaption(object):
    
    def __init__(self, path):
        with open(path) as f:
            self.json = json.load(f)
        if not self.json:
            self.json = []
            
    def _basic_text(self, drop_newline=False):
        '''
        Simply concatenate all the text together.
        '''
        lines = [x['text'] for x in self.json if 'text' in x]
        txt = "\n".join(lines)
        if drop_newline:
            txt = txt.replace('\n', ' ').replace('  ', ' ')
        return txt
    
    def _clean_text(self, text):
        # Nothing Done Yet
        return text
    
    @property
    def text(self):
        txt = self._basic_text(drop_newline=True)
        txt = self._clean_text(txt)
        return txt