{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus, dictionary\n",
    "import gensim.downloader as api\n",
    "from compare_tools.configuration import wem_loader\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from glove import Corpus, Glove\n",
    "from htrc_features import utils\n",
    "from motes_corpus.youtube import YTCaptionCorpus\n",
    "from motes_corpus.subtitles import SubtitleCorpus\n",
    "from motes_corpus.hathibook import HathiCorpus\n",
    "from motes_corpus import modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aligning with the GloVe wiki gigaword 300 model, which as 40000 *uncased* tokens, trained on 6B tokens from Wikipedia in 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = wem_loader('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab into a gensim dictionary format\n",
    "dict_path = '/data/motes/gigaword_300_dict.txt'\n",
    "if not os.path.exists(dict_path):\n",
    "    with open(dict_path, mode='w') as f:\n",
    "        f.write('1\\n')\n",
    "        for i, word in enumerate(model.vocab.keys()):\n",
    "            f.write('{}\\t{}\\t1\\n'.format(i, word))\n",
    "model_dict = dictionary.Dictionary.load_from_text(dict_path)\n",
    "n_words = len(model_dict)\n",
    "\n",
    "data_path = '/data/motes/'\n",
    "coocs_path = os.path.join(data_path, 'coocs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Cooccurrence Matrix\n",
    "\n",
    "GloVe uses a weighting of $(w/w_{max})^\\alpha$ for words that occur less than $w_{max}$. In Pennington et al 2014, they use $w_{max}=100$ and $\\alpha=\\frac{3}{4}$. This will be done later - for now we're just collecting raw cooccurrence counts.\n",
    "\n",
    "\n",
    "- Good guide: http://www.foldl.me/2014/glove-python/\n",
    "\n",
    "### Baseline English Wikipedia\n",
    "\n",
    "This will be the foundation on which the children's corpus is built.\n",
    "\n",
    "Since it is large, processing it in a notebook is unreasonable. Instead, see [train_enwiki.cooc.py](../training/train_enwiki.cooc.py)\n",
    "\n",
    "It was processed in batches, \n",
    "\n",
    "```bash\n",
    "python train_enwiki_cooc.py /data/motes/enwiki_cooc_full3_2_raw.npz -n 348268 -m 51732\n",
    "python train_enwiki_cooc.py /data/motes/enwiki_cooc_full4_raw.npz -n 400000 -m 600000\n",
    "...\n",
    "python train_enwiki_cooc.py /data/motes/enwiki_cooc_full12_raw.npz -n 6500000\n",
    "```\n",
    "\n",
    "..then combined into one big matrix:\n",
    "\n",
    "```python\n",
    "import glob\n",
    "paths = glob.glob('/data/motes/enwiki_cooc_full*') + ['/data/motes/coocs/enwiki_cooc_317k_raw.npz']\n",
    "all_dists = []\n",
    "validate = []\n",
    "for i, path in enumerate(paths):\n",
    "    partial_mat = sparse.load_npz(path)    \n",
    "    validate.append((path, partial_mat.sum()))\n",
    "    print(validate[-1])\n",
    "    \n",
    "    if i == 0:\n",
    "        full_mat = partial_mat\n",
    "    else:\n",
    "        full_mat += partial_mat\n",
    "sparse.save_npz('/data/motes/coocs/enwiki_full.npz', full_mat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full en wikipedia is 5.152919302929286x as big as the 317k pages in the CHI paper\n"
     ]
    }
   ],
   "source": [
    "a = full_mat.sum() / sparse.load_npz(os.path.join(coocs_path, 'enwiki_cooc_317k_raw.npz')).sum()\n",
    "print(\"The full en wikipedia is {}x as big as the 317k pages in the CHI paper\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus('/data/motes/simplewiki/simplewiki-latest-pages-articles.xml.bz2', lemmatize=False, dictionary=model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000, 14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500, 19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000, 23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500, 28000, 28500, 29000, 29500, 30000, 30500, 31000, 31500, 32000, 32500, 33000, 33500, 34000, 34500, 35000, 35500, 36000, 36500, 37000, 37500, 38000, 38500, 39000, 39500, 40000, 40500, 41000, 41500, 42000, 42500, 43000, 43500, 44000, 44500, 45000, 45500, 46000, 46500, 47000, 47500, 48000, 48500, 49000, 49500, 50000, 50500, 51000, 51500, 52000, 52500, 53000, 53500, 54000, 54500, 55000, 55500, 56000, 56500, 57000, 57500, 58000, 58500, 59000, 59500, 60000, 60500, 61000, 61500, 62000, 62500, 63000, 63500, 64000, 64500, 65000, 65500, 66000, 66500, 67000, 67500, 68000, 68500, 69000, 69500, 70000, 70500, 71000, 71500, 72000, 72500, 73000, 73500, 74000, 74500, 75000, 75500, 76000, 76500, 77000, 77500, 78000, 78500, 79000, 79500, 80000, 80500, 81000, 81500, 82000, 82500, 83000, 83500, 84000, 84500, 85000, 85500, 86000, 86500, 87000, 87500, 88000, 88500, 89000, 89500, 90000, 90500, 91000, 91500, 92000, 92500, 93000, 93500, 94000, 94500, 95000, 95500, 96000, 96500, 97000, 97500, 98000, 98500, 99000, 99500, 100000, 100500, 101000, 101500, 102000, 102500, 103000, 103500, 104000, 104500, 105000, 105500, 106000, \n",
      " 27047022 106434\n"
     ]
    }
   ],
   "source": [
    "word_n = 0\n",
    "docs = 0\n",
    "for i, text in enumerate(wiki.get_texts()):\n",
    "    word_n += len(text)\n",
    "    docs += 1\n",
    "    if i % 50000 == 0:\n",
    "        print(i, end=', ')\n",
    "print('\\n', word_n, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikicooc = modeling.train_coocurrence_matrix(wiki.get_texts(), model_dict,\n",
    "                                             window_size=10, print_every=2000)\n",
    "sparse.save_npz('/data/motes/simplewiki_cooc_raw.npz', wikicooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube\n",
    "\n",
    "Stats in CHI submission \\#1:\n",
    "\n",
    "```{'words': 9157014, 'docs': 15037, 'types': 265208}```\n",
    "\n",
    "Given the slow pace of collection from YouTube, we've kept the crawler going.\n",
    "\n",
    "Stats on Oct 9:\n",
    "\n",
    "```{'words': 28853789, 'docs': 41014, 'types': 507183}``` (3.15x words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8203 /data/motes/yt_captions/09-08/RYW5q6A1K24.json\n",
      "1 8203 /data/motes/yt_captions/09-08/5rZSTn2S_iU.json\n",
      "2 8203 /data/motes/yt_captions/09-08/JOdcsT6aD6A.json\n",
      "3 8203 /data/motes/yt_captions/09-08/Cw6FLD_SnkM.json\n",
      "4 8202 /data/motes/yt_captions/09-08/sTw-vEMLqWA.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    subset_of_paths = caption_paths[i::5]\n",
    "    print(i, len(subset_of_paths), subset_of_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs processed: 0\t time: 0s\t docs/second: 0\n",
      "Final docs processed: 820\t time: 11s\t docs/second: 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<400000x400000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2166334 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_of_paths = caption_paths[0::50]\n",
    "\n",
    "def glove_from_paths(paths, run_name, truncate_to=100000):\n",
    "    ytcorpus = YTCaptionCorpus(paths)\n",
    "    # TODO - save information connecting paths to name\n",
    "    cooc = modeling.train_coocurrence_matrix(ytcorpus.tokens(), model_dict,\n",
    "                                             window_size=10, print_every=2000)\n",
    "    # Truncate just to top n words in vocab\n",
    "    m = cooc.tocsc()[:truncate_to, :truncate_to].tocoo()\n",
    "\n",
    "    glove = Glove(no_components=300, learning_rate=0.05) \n",
    "    glove.fit(m, epochs=100, no_threads=25, verbose=True)\n",
    "    del m # not necessary, mainly for testing\n",
    "    dictionary = dict(list(model_dict.token2id.items())[:n])\n",
    "    glove.add_dictionary(dictionary)\n",
    "\n",
    "    # Convert to keyed vectors and save\n",
    "    kv = modeling.glove_to_keyedvectors(glove)\n",
    "    kv.save('/data/motes/models/{}.kv'.format(run_name))\n",
    "    \n",
    "glove_from_paths(subset_of_paths, 'r1_split1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>run_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data/motes/yt_captions/09-08/RYW5q6A1K24.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data/motes/yt_captions/09-08/fLRTMXlfCuU.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data/motes/yt_captions/09-08/lVLnUrbM-dU.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data/motes/yt_captions/09-08/BaEziR2X4Aw.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data/motes/yt_captions/09-08/m_zeTomaJJo.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>/data/motes/yt_captions/10-07/GjMdC2H3qCc.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>/data/motes/yt_captions/10-07/wvQN5fUZG3Q.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>/data/motes/yt_captions/10-07/asLU2NjEUZw.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>/data/motes/yt_captions/10-07/yFccT_HP2Js.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>/data/motes/yt_captions/10-07/9sRIqTFzMgc.json</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             fnames run_name\n",
       "0    /data/motes/yt_captions/09-08/RYW5q6A1K24.json     test\n",
       "1    /data/motes/yt_captions/09-08/fLRTMXlfCuU.json     test\n",
       "2    /data/motes/yt_captions/09-08/lVLnUrbM-dU.json     test\n",
       "3    /data/motes/yt_captions/09-08/BaEziR2X4Aw.json     test\n",
       "4    /data/motes/yt_captions/09-08/m_zeTomaJJo.json     test\n",
       "..                                              ...      ...\n",
       "816  /data/motes/yt_captions/10-07/GjMdC2H3qCc.json     test\n",
       "817  /data/motes/yt_captions/10-07/wvQN5fUZG3Q.json     test\n",
       "818  /data/motes/yt_captions/10-07/asLU2NjEUZw.json     test\n",
       "819  /data/motes/yt_captions/10-07/yFccT_HP2Js.json     test\n",
       "820  /data/motes/yt_captions/10-07/9sRIqTFzMgc.json     test\n",
       "\n",
       "[821 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(subset_of_paths, columns=['fnames'])\n",
    "df['run_name'] = 'test'\n",
    "df.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_to = 200000\n",
    "#fpath = '/data/motes/coocs/enwiki_full.npz'\n",
    "for fpath in ['/data/motes/coocs/all_weighted_10-12.npz']:\n",
    "    just_name = os.path.splitext(os.path.split(fpath)[-1])[0]\n",
    "\n",
    "    # Truncate just to top n words in vocab\n",
    "    m = cooc.tocsc()[:truncate_to, :truncate_to].tocoo()\n",
    "\n",
    "    glove = Glove(no_components=300, learning_rate=0.05) \n",
    "    glove.fit(m, epochs=100, no_threads=25, verbose=True)\n",
    "    del m # not necessary, mainly for testing\n",
    "    dictionary = dict(list(model_dict.token2id.items())[:n])\n",
    "    glove.add_dictionary(dictionary)\n",
    "\n",
    "    # Convert to keyed vectors and save\n",
    "    kv = modeling.glove_to_keyedvectors(glove)\n",
    "    kv.save('/data/motes/models/{}_{}k.kv'.format(just_name, int(n/1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': 28853789, 'docs': 41014, 'types': 507183}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_paths = glob.glob('/data/motes/yt_captions/**/*')\n",
    "\n",
    "ytcorpus = YTCaptionCorpus(caption_paths)\n",
    "ytcorpus.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs processed: 0\t time: 0s\t docs/second: 0\n",
      "Docs processed: 2000\t time: 12s\t docs/second: 161\n"
     ]
    }
   ],
   "source": [
    "cooc = modeling.train_coocurrence_matrix(ytcorpus.tokens(), model_dict,\n",
    "                                         window_size=10, print_every=2000)\n",
    "sparse.save_npz(os.path.join(coocs_path, 'yt_cooc_raw_{}.npz'.format(time.strftime('%m-%d'))), cooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': 12436065, 'docs': 5737, 'types': 493642}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_paths = glob.glob('/data/motes/show_subtitles/**/*')\n",
    "showcorpus = SubtitleCorpus(subtitle_paths)\n",
    "showcorpus.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_paths = glob.glob('/data/motes/show_subtitles/**/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(subtitle_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2869"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subtitle_paths[0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/motes/show_subtitles/The New Batman Adventures/The New Batman Adventures - 01x05 - You Scratch My Back.srt',\n",
       " '/data/motes/show_subtitles/Transformers: Robots in Disguise (US)/Transformers Robots in Disguise (US) - 04x20 - Prisoners Principles.srt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs processed: 0\t time: 0s\t docs/second: 0\n",
      "Docs processed: 2000\t time: 708s\t docs/second: 2\n"
     ]
    }
   ],
   "source": [
    "cooc = modeling.train_coocurrence_matrix(showcorpus.tokens(), model_dict,\n",
    "                                         window_size=10, print_every=2000)\n",
    "sparse.save_npz(os.path.join(coocs_path, 'shows_cooc_raw_{}.npz'.format(time.strftime('%m-%d'))), cooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Children's Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/extracted-features-parquet-stubby/uc1/b88/uc1.b4088188.tokens.parquet'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kidsbooks = pd.read_parquet('/data/motes/kids_books.parquet')\n",
    "book_paths = (['/data/extracted-features-parquet-stubby/{}.tokens.parquet'.format(utils.id_to_stubbytree(htid)) for htid in kidsbooks.htid])\n",
    "book_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'words': 5817930, 'docs': 22465, 'types': 104704}, 581793000.0, 2246500.0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate based on 1% of the corpus\n",
    "random.shuffle(book_paths)\n",
    "sample_prop = .01\n",
    "sample_size = int(len(book_paths) * sample_prop)\n",
    "htcorpus = HathiCorpus(book_paths[:sample_size])\n",
    "s = htcorpus.stats()\n",
    "s, s['words']/sample_prop, s['docs']/sample_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HathiTrust Children's books had to be processed in parallel because of size, using `scripts/ht_cooc_matrix.py`, so those need to be combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 28.6 s, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for name, lastdir in [('2005-10', '2005'), ('2000-05', '2000')]:\n",
    "    cooc = modeling.merge_sparse_matrices(glob.glob('/data/motes/ht_coocs/{}/*'.format(lastdir)))\n",
    "    sparse.save_npz(os.path.join(coocs_path, 'ht_coocs/ht_cooc_raw_{}'.format(name)), cooc)\n",
    "\n",
    "# Combine all the decade matrices\n",
    "#cooc = modeling.merge_sparse_matrices(glob.glob('/data/motes/ht_coocs/ht_cooc_raw_1*'))\n",
    "#sparse.save_npz('/data/motes/ht_coocs/ht_cooc_raw_all', cooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine matrixes with weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/motes/simplewiki_cooc_raw.npz \t 13086210.66666866\n",
      "/data/motes/yt_cooc_raw_09-07.npz \t 12011316.945634851\n",
      "/data/motes/shows_cooc_raw_09-07.npz \t 36357764.06587125\n",
      "/data/motes/ht_coocs/ht_cooc_raw_2005-10.npz \t 10457207.777608508\n",
      "/data/motes/ht_coocs/ht_cooc_raw_2000-05.npz \t 8335299.301917659\n",
      "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz \t 113350320.77022566\n",
      "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz \t 13482573.579135073\n",
      "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz \t 14833244.88504179\n"
     ]
    }
   ],
   "source": [
    "# Get a sense of how big various matrixes are\n",
    "paths = glob.glob('/data/motes/*npz') + glob.glob(os.path.join(coocs_path, 'ht_coocs/ht_cooc_raw_[12]*'))\n",
    "for path in paths:\n",
    "    print(path, '\\t', sparse.load_npz(path).tocsr()[:100,:100].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distributions, if eliminating all cooccurrences below 1. As we see below, ther distributions end up being very similar for the Bag-of-words tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0100</th>\n",
       "      <th>0.0500</th>\n",
       "      <th>0.1000</th>\n",
       "      <th>0.2000</th>\n",
       "      <th>0.3000</th>\n",
       "      <th>0.4000</th>\n",
       "      <th>0.5000</th>\n",
       "      <th>0.6000</th>\n",
       "      <th>0.7000</th>\n",
       "      <th>0.8000</th>\n",
       "      <th>0.9000</th>\n",
       "      <th>0.9500</th>\n",
       "      <th>0.9900</th>\n",
       "      <th>0.9970</th>\n",
       "      <th>0.9990</th>\n",
       "      <th>0.9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/data/motes/simplewiki_cooc_raw.npz</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.33</td>\n",
       "      <td>7.00</td>\n",
       "      <td>13.67</td>\n",
       "      <td>67.67</td>\n",
       "      <td>210.00</td>\n",
       "      <td>554.67</td>\n",
       "      <td>3459.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/motes/yt_cooc_raw_09-07.npz</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.80</td>\n",
       "      <td>6.00</td>\n",
       "      <td>13.23</td>\n",
       "      <td>28.80</td>\n",
       "      <td>158.70</td>\n",
       "      <td>535.39</td>\n",
       "      <td>1597.83</td>\n",
       "      <td>10706.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/motes/shows_cooc_raw_09-07.npz</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.70</td>\n",
       "      <td>15.80</td>\n",
       "      <td>36.50</td>\n",
       "      <td>232.00</td>\n",
       "      <td>858.13</td>\n",
       "      <td>2688.03</td>\n",
       "      <td>26113.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.30</td>\n",
       "      <td>4.77</td>\n",
       "      <td>7.91</td>\n",
       "      <td>18.44</td>\n",
       "      <td>41.60</td>\n",
       "      <td>245.32</td>\n",
       "      <td>853.54</td>\n",
       "      <td>2527.43</td>\n",
       "      <td>22068.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.05</td>\n",
       "      <td>4.28</td>\n",
       "      <td>6.83</td>\n",
       "      <td>14.93</td>\n",
       "      <td>31.63</td>\n",
       "      <td>167.89</td>\n",
       "      <td>549.61</td>\n",
       "      <td>1591.98</td>\n",
       "      <td>10823.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz</th>\n",
       "      <td>1.01</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.27</td>\n",
       "      <td>6.81</td>\n",
       "      <td>14.85</td>\n",
       "      <td>31.45</td>\n",
       "      <td>165.43</td>\n",
       "      <td>539.70</td>\n",
       "      <td>1540.72</td>\n",
       "      <td>11077.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0.0100  0.0500  0.1000  \\\n",
       "/data/motes/simplewiki_cooc_raw.npz               1.00    1.00    1.00   \n",
       "/data/motes/yt_cooc_raw_09-07.npz                 1.00    1.00    1.00   \n",
       "/data/motes/shows_cooc_raw_09-07.npz              1.00    1.00    1.00   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz    1.01    1.07    1.15   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz      1.01    1.07    1.14   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz    1.01    1.07    1.14   \n",
       "\n",
       "                                                0.2000  0.3000  0.4000  \\\n",
       "/data/motes/simplewiki_cooc_raw.npz               1.00    1.00    1.00   \n",
       "/data/motes/yt_cooc_raw_09-07.npz                 1.20    1.40    1.70   \n",
       "/data/motes/shows_cooc_raw_09-07.npz              1.20    1.40    1.70   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz    1.34    1.60    1.96   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz      1.32    1.56    1.87   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz    1.32    1.55    1.87   \n",
       "\n",
       "                                                0.5000  0.6000  0.7000  \\\n",
       "/data/motes/simplewiki_cooc_raw.npz               1.33    1.67    2.33   \n",
       "/data/motes/yt_cooc_raw_09-07.npz                 2.10    2.70    3.80   \n",
       "/data/motes/shows_cooc_raw_09-07.npz              2.10    2.80    4.00   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz    2.48    3.30    4.77   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz      2.34    3.05    4.28   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz    2.33    3.04    4.27   \n",
       "\n",
       "                                                0.8000  0.9000  0.9500  \\\n",
       "/data/motes/simplewiki_cooc_raw.npz               3.33    7.00   13.67   \n",
       "/data/motes/yt_cooc_raw_09-07.npz                 6.00   13.23   28.80   \n",
       "/data/motes/shows_cooc_raw_09-07.npz              6.70   15.80   36.50   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz    7.91   18.44   41.60   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz      6.83   14.93   31.63   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz    6.81   14.85   31.45   \n",
       "\n",
       "                                                0.9900  0.9970   0.9990  \\\n",
       "/data/motes/simplewiki_cooc_raw.npz              67.67  210.00   554.67   \n",
       "/data/motes/yt_cooc_raw_09-07.npz               158.70  535.39  1597.83   \n",
       "/data/motes/shows_cooc_raw_09-07.npz            232.00  858.13  2688.03   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz  245.32  853.54  2527.43   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz    167.89  549.61  1591.98   \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz  165.43  539.70  1540.72   \n",
       "\n",
       "                                                  0.9999  \n",
       "/data/motes/simplewiki_cooc_raw.npz              3459.20  \n",
       "/data/motes/yt_cooc_raw_09-07.npz               10706.78  \n",
       "/data/motes/shows_cooc_raw_09-07.npz            26113.54  \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1850-1980.npz  22068.87  \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1980-90.npz    10823.81  \n",
       "/data/motes/ht_coocs/ht_cooc_raw_1990-2000.npz  11077.11  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dists = []\n",
    "quantiles = [.01,.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,.99, .997, .999, .9999]\n",
    "for path in paths:\n",
    "    m = sparse.load_npz(path)\n",
    "    m.data[m.data < 1] = 0\n",
    "    m.eliminate_zeros()\n",
    "    a = np.quantile(m.data, quantiles)\n",
    "    del m\n",
    "    b = pd.Series(dict(zip(quantiles, a)))\n",
    "    b.name = path\n",
    "    all_dists.append(b)\n",
    "pd.DataFrame(all_dists).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32454196.693552487 0.1 25847556436.226795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Combine with the OG EN Wiki\n",
    "# Load Sparse Matrix\n",
    "cooc = sparse.load_npz(os.path.join(coocs_path, 'enwiki_full.npz'))\n",
    "print(cooc.data.max(), cooc.data.min(), cooc.data.sum())\n",
    "\n",
    "# Shrink by factor of 10 and drop bottom 20% of (rare) coocs\n",
    "cooc.data /= 10\n",
    "cutoff = np.quantile(cooc.data, .2)\n",
    "cooc.data[cooc.data < cutoff] = 0\n",
    "cooc.eliminate_zeros()\n",
    "\n",
    "margins = np.array(cooc.sum(0))[0]\n",
    "\n",
    "ordered_weights = [('simplewiki_cooc_raw.npz', .5, .2),\n",
    "                   ('yt_cooc_raw_10-10.npz', 1, .2),\n",
    "                   ('shows_cooc_raw_09-07.npz', 1, .2),\n",
    "                   ('ht_coocs/ht_cooc_raw_1850-1980.npz', .1, 1),\n",
    "                   ('ht_coocs/ht_cooc_raw_1980-90.npz', .2, 1),\n",
    "                   ('ht_coocs/ht_cooc_raw_1990-2000.npz', .3, 1),\n",
    "                   ('ht_coocs/ht_cooc_raw_2000-05.npz', .4, 1),\n",
    "                   ('ht_coocs/ht_cooc_raw_2005-10.npz', .5, 1),\n",
    "                  ]\n",
    "\n",
    "for i, (path, weight, drop_below) in enumerate(ordered_weights):\n",
    "    path = os.path.join(coocs_path, path)\n",
    "    partial_mat = sparse.load_npz(path)\n",
    "    # Scale to same size as enwiki, then scale\n",
    "    size_diff = (margins[:200] / np.array(partial_mat.sum(0))[0][:200])\n",
    "    scaling = weight * size_diff[~np.isnan(size_diff) & (size_diff != np.inf)].mean()\n",
    "    if drop_below:\n",
    "        # Drop really low coocurrences\n",
    "        partial_mat.data[partial_mat.data < drop_below] = 0\n",
    "        partial_mat.eliminate_zeros()\n",
    "    cooc += (partial_mat * scaling)\n",
    "sparse.save_npz(os.path.join(coocs_path, 'all_weighted_{}.npz'.format(time.strftime('%m-%d'))), cooc)\n",
    "#sparse.save_npz(os.path.join(coocs_path, 'yt_w_en_{}.npz'.format(time.strftime('%m-%d'))), cooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0100        0.200000\n",
       "0.0500        0.400000\n",
       "0.1000        0.536830\n",
       "0.2000        0.666667\n",
       "0.3000        0.674892\n",
       "0.4000        0.900000\n",
       "0.5000        1.000000\n",
       "0.6000        1.333333\n",
       "0.7000        2.000000\n",
       "0.8000        3.249331\n",
       "0.9000        8.120466\n",
       "0.9500       19.700000\n",
       "0.9600       25.892300\n",
       "0.9700       36.499497\n",
       "0.9800       58.628357\n",
       "0.9900      128.877679\n",
       "0.9970      475.963757\n",
       "0.9990     1493.601192\n",
       "0.9999    14222.358332\n",
       "dtype: float64"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.quantile(full_mat.data, quantiles)\n",
    "b = pd.Series(dict(zip(quantiles, a)))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate\n",
    "\n",
    "If we were to truncate the vocabulary at $n$, what % of cooccurrences would be retained vs dropped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhTklEQVR4nO3de3xcdZ3/8dcn9+bWpG2atkna9AZtWtpSQrm5rMqCBfnJys9FYFHkYpdVWNfVH4vs4+fqY9eV1V0VH6uW/hAERVAUV5RKi6yIAtKm9zv0Rpu0TVKa5tLck8/vjzltp2nSTJvLTGbez8djHnPme74z85nT5p1vvufMOebuiIhI4kiKdgEiIjK8FPwiIglGwS8ikmAU/CIiCUbBLyKSYFKiXUBvxo0b56WlpdEuQ0RkxFizZs1hdy+IpG9MBn9paSkVFRXRLkNEZMQws3ci7aupHhGRBKPgFxFJMAp+EZEEo+AXEUkwCn4RkQTTb/Cb2WNmVmNmm/tYb2b2bTPbaWYbzWxh2LrFZrYjWPfAYBYuIiLnJpIR/w+AxWdYfy0wM7gtAb4HYGbJwHeC9WXALWZWNpBiRURk4PoNfnd/FThyhi43AE96yJ+APDObCCwCdrr7bndvB54J+oqISA9r99XxyO93Dct7DcYcfxGwP+xxZdDWV3uvzGyJmVWYWUVtbe0glCUiMjL8cn0VNy/7Ez9etY9jbZ1D/n6DEfzWS5ufob1X7r7M3cvdvbygIKJvHYuIjGjd3c43Vu7gM8+sZ0FJHv/9qSvISh/6EyoMxjtUAiVhj4uBA0BaH+0iIgmvpb2Lzz+7gRc2HeSm8mL+9S8vIC1leA60HIzgfx6418yeAS4B6t39oJnVAjPNbCpQBdwM3DoI7yciMqJVN7TyyScr2FRVz4PXzeKTfzYNs94mSYZGv8FvZk8D7wXGmVkl8M9AKoC7LwWWA9cBO4Fm4I5gXaeZ3QusAJKBx9x9yxB8BhGREWNzVT13P1FBQ2sHyz5WztVlhcNeQ7/B7+639LPegU/3sW45oV8MIiIJ78XNB/nsTzaQn5nKz+65nLJJuVGpIyZPyywiEk/cne++souvr9jBhZPzeORjFzE+JyNq9Sj4RUSGUFtnF194bhPPra3iQ/Mn8bWPzCMjNTmqNSn4RUSGyLtNbfzND9dQ8U4d/3D1edz3/hnDuhO3Lwp+EZEhsONQI3c9sZraxjb+69YLuX7epGiXdIKCX0RkkP1uew33Pb2OzLRkfvo3lzG/JC/aJZ1CwS8iMkjcncde28tXXtjK7Im5PHp7ORNHj4p2WadR8IuIDIKOrm6++MstPL1qH4vnTOAbH51PZlpsRmxsViUiMoIcbW7nU0+t5fVd7/Lp903nc1efT1JS9Hfi9kXBLyIyALtrm7jriQqq6lr4xk3zuXFhcbRL6peCX0TkHL2+8zB/+9RaUpKMH3/yEspLx0S7pIgo+EVEzsGP39zHF3+5mWkFWXz/9ospGZMZ7ZIipuAXETkLXd3Ov76wlcdf28v7zi/g27dcSE5GarTLOisKfhGRCDW2dnDf0+t4ZUctd14xlX/64GySY3gnbl8U/CIiEdh/pJm7nljN7tpj/NuHL+DWSyZHu6RzpuAXEelHxd4jLPnhGrq6nSfvXMTlM8ZFu6QBUfCLiJzBc2sreeDnmyjOH8Wjt5czrSA72iUNmIJfRKQX3d3Of6zcwXdf2cXl08fyvb++iNGZI2snbl8U/CIiPTS3d/LZn6xnxZZqbr1kMl/+0BxSk4fnQujDQcEvIhLmYH0Ldz9RwbaDDXzx+jLuuKI0Js6hP5gU/CIigQ37j3L3kxW0tHfx/U9czPvOHx/tkoaEgl9EBPj1xgN87qcbKMhJ56m7L+G8wpxolzRkFPwiktDcnW+/vJNv/vYtyqfk88jHLmJsdnq0yxpSCn4RSVitHV3c/7ONPL/hADcuLOKrN15Aekp0L4Q+HBT8IpKQahpbWfLkGtbvP8r9i8/nb/98etztxO2Lgl9EEs7WAw3c/cRq6po7WHrbRSyeOyHaJQ0rBb+IJJSXtlbzmWfWkZuRyrP3XMbcotHRLmnYRfSNBDNbbGY7zGynmT3Qy/p8M/uFmW00s1VmNjds3WfNbIuZbTazp80sYzA/gIhIJNydR36/iyU/rGDG+Gx+ee8VCRn6EMGI38ySge8AVwOVwGoze97dt4Z1exBY7+4fNrNZQf+rzKwI+DugzN1bzOynwM3ADwb5c4hIHHJ32ru6aW7r4lh7J83tXRxr63Hf3klzWxfN7V00t3eeeNyzf2NrJ1VHW/jgvIn8x0fmMyot/nfi9iWSqZ5FwE533w1gZs8ANwDhwV8GfBXA3bebWamZFYa9xygz6wAygQODVbyIxI7ubqe5o4vmtk6O9RLMofvQupae7X31b++iq9sjriEjNYmstBQy05ND92nJZKWnUJCTTlZaCvOKR/Pxy0pj+kLowyGS4C8C9oc9rgQu6dFnA3Aj8EczWwRMAYrdfY2Z/QewD2gBVrr7yoGXLSJDqaOrm6PNHdQ1t1N3rJ265g6ONrdzpLk91N5L29HmdiLN6CSDrPQgmIOgzkxLYVx2GpPTM8lKCz3OCtqz0pLJTE85LdSPB3tm0H8kXhQlGiIJ/t62ZM9/3oeAh81sPbAJWAd0mlk+ob8OpgJHgWfN7DZ3/9Fpb2K2BFgCMHnyyL3AgUisaWnv4kgQ4MfD/GhzO0eOnVyuOx7yze0cPdZBY1tnn6+XlpJEfmYq+Zlp5GemMWtCDnmZaeRnppKbkUpWenhgnz76zkxLJj0lKWEOnYxFkQR/JVAS9riYHtM17t4A3AFgoX/NPcHtA8Aed68N1j0HXA6cFvzuvgxYBlBeXh7533YiCcLdaWjtDEbbJ0P8yCmBfnpbW2d3n6+Zk55CXlYoxPMy05g6LutEoOdnpZ4I9NDj0PKo1GSF9ggXSfCvBmaa2VSgitDO2VvDO5hZHtDs7u3A3cCr7t5gZvuAS80sk9BUz1VAxSDWLzLiuTsNLZ1UN7ZS3dBKdUMbNY2t1DS0BY9DbbWNbbR39R7iSQZ5mWnkBSFdnD+KC4pGk591si08wPMyU8kblUZaSvycalgi12/wu3unmd0LrACSgcfcfYuZ3ROsXwrMBp40sy5CO33vCta9aWY/A9YCnYSmgJYNyScRiTHuTlNbZyjIG1qpPhHmbcFyKNCrG1p7HZXnZKRQmJtBYW46l0wdQ0FuOgXZ6eRlpjHmxGj85BRLou+wlMiZe+zNqpSXl3tFhf4wkNjV3N55IrSrG1qpbWw7MTKvbmilJnjc3N512nMz05KZkJvB+Nz0INgzGJ+TzvjcDApzQm3jc9PJTNP3KyVyZrbG3csj6av/WSJhWju6QqPyM0y71DS09brzMyM1KRTkORnMmZTL+2eNpzAI94KckyGfna4fO4ku/Q+UhNPd7VQdbWFXbRO7ao+F7mua2FXbxOGm9tP6pyUnnRidnz8hhz+bWXBiCmZ8TnCfm0FuRop2esqIoOCXuNXa0cWew8fYGYT6rtpj7KppYvfhJlo7Ts6p52WmMqMgm6tmFVIyZtSJkfnxKZi8zFQFusQVBb+MaO7OkWPtJ0buJ0O+icq6Fo7vwjKD4vxRTC/I5vLpY5k+PpvpBdlML8hiTFaagl0SioJfRoSubmf/keYTob6r5hg7g+WjzR0n+mWkJjFtXDYLSvL53wuLmREE/NRxWWSkJu65WUTCKfglphxr62T38Xn3sJDfc/jYKcewj8tOZ3pBFtddMPHEyH3G+GwmjR6lwxpF+qHgl2Hn7tQ2toVG7DWn7mA9UN96ol9ykjF5TCbTC7J576yCIOBDIZ+XmRbFTyAysin4Zci1d3bzp93v8tLWajZW1bO7pumUwyGz0pKZPj6bS6aNDaZmsphekM3ksZkJcf1TkeGm4Jch0djawSs7alm5tZpXttfQ2NZJZloyC0ry+PDCIqYXZJ+Yfy/MTdfOVZFhpOCXQVPd0MpLW6tZubWaN3YdpqPLGZedxgfnTeSaOYVcPn2cdrCKxAAFv5wzd2dXbRMrtoTCfsP+owCUjs3kziumcs2cQhaU5Osc6SIxRsEvZ6Wr21m/v46VQdjvOXwMgPklefyfD5zPNWWFzBifrakbkRim4Jd+tXZ08fquw6zcUs1vt1VzuKmd1GTjsunjuPM9U7l6diETRmdEu0wRiZCCX3pV39zB/+yoZuWWan7/Vi3N7V1kp6fwvlnjuaaskD8/v4DcjNRolyki50DBLydUHW3hpS2HWLm1mjf3HKGr2ynMTefGhUVcXTaBS6eN0eGVInFAwZ/A3J3thxqD+fpDbDnQAMDM8dn8zZXTuGbOBOYVjdY3YUXijII/wXR2dVPxTmjn7EvbDrH/SAtmcNHkfL5w7SyuLitkWkF2tMsUkSGk4E8ALe1dvPp2LS9treblbdXUNXeQlpLEe2aM49PvncFVswspyEmPdpkiMkwU/HHq3aY2Xt5ew0tbq/nD27W0dnSTm5HCVbMLuaaskCvPKyBLV4ISSUj6yY8j+480syLYOVux9wjdDpNGZ3DzxZO5pqyQi6eOITU5KdplikiUKfhHOHfnpa3VPP7aXt7Y/S4AsybkcO/7Z3JNWSFzJuXqy1QicgoF/wjl7qzcWs23fvs22w42UJw/is9dfR43LChi8tjMaJcnIjFMwT8CvbbzMP/+4nY2VtYzdVwW37hpPh+aP4kUTeOISAQU/CPI6r1H+M+VO/jT7iMU5Y3i6x+Zx4cvLFLgi8hZUfCPAM3tnXztxR384PW9jM9J54vXl3HrJZN1imMROScK/hjm7vx8bRVfX7Gd6oY27riilPs/MItRaQp8ETl3Cv4Y9W5TG/f/bCMvb69hfkke3/3rhVw0ZUy0yxKROBBR8JvZYuBhIBl41N0f6rE+H3gMmA60Ane6++ZgXR7wKDAX8GDdG4P1AeKNu/OLdVX8y6+3cqytiy9eX8YnLi/V+XJEZND0G/xmlgx8B7gaqARWm9nz7r41rNuDwHp3/7CZzQr6XxWsexh40d0/YmZpgI417EN9Swf/+LONvLjlEAsn5/HVG+dx/oScaJclInEmkhH/ImCnu+8GMLNngBuA8OAvA74K4O7bzazUzAqBFuBK4BPBunagfdCqjyNr3qnjM8+s41B9Kw9eN4u73zNNo3wRGRKRHAdYBOwPe1wZtIXbANwIYGaLgClAMTANqAUeN7N1ZvaomWX19iZmtsTMKsysora29iw/xsjV3e1875Vd3PRIaPbrp/dcxpIrpyv0RWTIRBL8vSWQ93j8EJBvZuuB+4B1QCehvygWAt9z9wuBY8ADvb2Juy9z93J3Ly8oKIiw/JGtqa2Tu55Yzb+/uJ3Fcybwwt/9GQsn50e7LBGJc5FM9VQCJWGPi4ED4R3cvQG4A8BCJ4bZE9wygUp3fzPo+jP6CP5EU3esnU88vorNBxr4lxvmcNulU3ROHREZFpEE/2pgpplNBaqAm4FbwzsER+40B3P4dwOvBr8MGsxsv5md7+47CO3w3UqC23GokXt+tIaqoy08cttF/EVZYbRLEpEE0m/wu3unmd0LrCB0OOdj7r7FzO4J1i8FZgNPmlkXoWC/K+wl7gOeCo7o2U3wl0GienHzQT77kw1kZ6Tw1N2XcHGpjs0XkeFl7j2n66OvvLzcKyoqol3GoHv0D7v5yvJtLCjJ45HbLmJ8bka0SxKROGFma9y9PJK++ubuMFn6+1089JvtXDt3At/86AKdZ0dEokbBPwwef20PD/1mO/9r/iS+edN8nU1TRKJKCTTE/ntdFV/+1VY+MKdQoS8iMUEpNIR+/1Ytn392A5dOG8PDN1+o0BeRmKAkGiLr9x/lb3+0hvMKc1j28XLN6YtIzFDwD4E9h49x5w9WMzY7jR/ceTG5GanRLklE5AQF/yB7t6mNTzy+CnfniTsWMT5Hh2yKSGzRUT2DqKOrm089tZZD9a08veRSphVkR7skEZHTKPgH0Vde2Mabe47wzY/O18nWRCRmaapnkLy4+SA/eH0vd14xlQ9fWBztckRE+qTgHwQH61v4x59vYl7xaB64dla0yxEROSMF/wC5Ow8+t4mOrm4evvlC0lK0SUUktimlBujFzYf43Y5aPnfN+Uwd1+vFxUREYoqCfwCa2jr50q+2UDYxl9svmxLtckREIqKjegbg4d++RXVDG0tvu0inYxCREUNpdY7erm7k8df2cvPFJVyoQzdFZARR8J8Dd+efn99CVnoK9y/WUTwiMrIo+M/B8k2HeH3Xu3z+mvMYk5UW7XJERM6Kgv8sNbd38pUXtlI2MZdbL9EOXREZeRT8Z2npK7s4UN/Kl2+YQ3KSRbscEZGzpuA/C/uPNPPIq7u5YcEkLi4dE+1yRETOiYL/LHz1N9tIMtNpGURkRFPwR2jrgQaWbzrEJ6+cxsTRo6JdjojIOVPwR+i7r+wkOz2Fu66YGu1SREQGRMEfgd21Tbyw6SC3XTqF0Zm6jKKIjGwK/gg8+sc9pCUncdd7NNoXkZEvouA3s8VmtsPMdprZA72szzezX5jZRjNbZWZze6xPNrN1ZvbrwSp8uNQ3d/Dc2kr+ckERBTnp0S5HRGTA+g1+M0sGvgNcC5QBt5hZWY9uDwLr3X0e8HHg4R7rPwNsG3i5w+8nFfto7ejm9stLo12KiMigiGTEvwjY6e673b0deAa4oUefMuBlAHffDpSaWSGAmRUDHwQeHbSqh0lXt/PkG++waOoYyiblRrscEZFBEUnwFwH7wx5XBm3hNgA3ApjZImAKcPzCs98C7ge6z/QmZrbEzCrMrKK2tjaCsobeS1urqaxr4Q6N9kUkjkQS/L2dl8B7PH4IyDez9cB9wDqg08yuB2rcfU1/b+Luy9y93N3LCwoKIihr6H3/j7spGTOKa+ZMiHYpIiKDJpILsVQCJWGPi4ED4R3cvQG4A8DMDNgT3G4GPmRm1wEZQK6Z/cjdbxuE2ofU+v1HWb23jv97fZnOySMicSWSEf9qYKaZTTWzNEJh/nx4BzPLC9YB3A286u4N7v4Fdy9299Lgef8zEkIf4PHX9pCTnsJN5cX9dxYRGUH6HfG7e6eZ3QusAJKBx9x9i5ndE6xfCswGnjSzLmArcNcQ1jzk6ps7+M3mQ9xycQk5GfrClojEl4iuuevuy4HlPdqWhi2/Aczs5zVeAV456wqj4PkNVbR3dvNX5SX9dxYRGWH0zd1ePLumktkTc5lbNDrapYiIDDoFfw87DjWysbKev7pIc/siEp8U/D08t66SlCTjhgWTol2KiMiQUPCH6e52frX+AFeeV8DYbJ2XR0Tik4I/zOq9RzhQ36rRvojENQV/mP9ef4DMtGSuLiuMdikiIkNGwR9o7ehi+aaDXF1WSGZaREe5ioiMSAr+wIubD1Hf0sFNOnZfROKcgj/w41X7mDI2k8umjY12KSIiQ0rBD+yqbWLVniN89OISknRCNhGJcwp+4NmK0LH7H9GXtkQkASR88Ls7yzcd5PIZ4xifkxHtckREhlzCB//Wgw3sO9LMtXN1sRURSQwJH/wrNh8iydCx+yKSMBI++H+z+RAXl45hnE7RICIJIqGDf1dtE2/XNGmaR0QSSkIH/8vbqgH4C03ziEgCSejg/+22GmZNyKE4PzPapYiIDJuEDf6jze2seaeOv5it0b6IJJaEDf5XdtTS1e1cNXt8tEsRERlWCRv8L2+vYVx2OvOL86JdiojIsErI4O/udl7beZgrZ47TuXlEJOEkZPC/VdPIkWPtXD5jXLRLEREZdgkZ/K/vfBeAy6brFMwikngSM/h3vcuUsZkU5Y2KdikiIsMu4YK/q9t5c8+7XK7RvogkqIiC38wWm9kOM9tpZg/0sj7fzH5hZhvNbJWZzQ3aS8zsd2a2zcy2mNlnBvsDnK0tB+ppbO3ksuma3xeRxNRv8JtZMvAd4FqgDLjFzMp6dHsQWO/u84CPAw8H7Z3A59x9NnAp8OlenjusVu05AsClU8dEswwRkaiJZMS/CNjp7rvdvR14BrihR58y4GUAd98OlJpZobsfdPe1QXsjsA0oGrTqz8G6fUcpzh/F+FxddEVEElMkwV8E7A97XMnp4b0BuBHAzBYBU4BTrmNoZqXAhcCb51jroFi3r44LJ+dHswQRkaiKJPh7+4aT93j8EJBvZuuB+4B1hKZ5Qi9glg38HPh7d2/o9U3MlphZhZlV1NbWRlL7WTtU38qB+lYWTs4bktcXERkJUiLoUwmUhD0uBg6EdwjC/A4AMzNgT3DDzFIJhf5T7v5cX2/i7suAZQDl5eU9f7EMirX76gA04heRhBbJiH81MNPMpppZGnAz8Hx4BzPLC9YB3A286u4NwS+B7wPb3P0bg1n4uVi3r470lCTKJuZGuxQRkajpd8Tv7p1mdi+wAkgGHnP3LWZ2T7B+KTAbeNLMuoCtwF3B068APgZsCqaBAB509+WD+zEis3bfUS4oGk1aSsJ9fUFE5IRIpnoIgnp5j7alYctvADN7ed4f6X0fwbDr6Opmc1U9H7t0SrRLERGJqoQZ+r5d3URbZzfzSvKiXYqISFQlTPBvqjoKwAVFo6NbiIhIlCVQ8NeTk57ClDG6vq6IJLbECf7KeuYWjdaFV0Qk4SVE8Ld3drPtUCMXFGuaR0QkIYL/repG2ju7mav5fRGRxAj+zVX1gHbsiohAggS/duyKiJyUMMGvHbsiIiFxH/ztnd1sP6gduyIix8V98L9V3Uh7l3bsiogcF/fBf3zH7jwFv4gIkADBv6mqnpyMFKaM1Y5dERFIkOCfO2k0oUsDiIhIXAe/duyKiJwuroN/Z00T7V3dzJmkK26JiBwX18H/dk0jAOdPyIlyJSIisSOug39nTRPJScbUcVnRLkVEJGbEdfC/Vd3IlLGZpKckR7sUEZGYEdfB/3ZNEzPHZ0e7DBGRmBK3wd/W2cU77zZzXqHm90VEwsVt8O85fIyubmeGRvwiIqeI2+B/u7oJgJnjNeIXEQkXv8Ff00SSwbQCHdEjIhIufoO/upEpY7PISNURPSIi4eI2+PccPsY0Hb8vInKauAx+d6eqroUSXWpRROQ0EQW/mS02sx1mttPMHuhlfb6Z/cLMNprZKjObG+lzh0JDSyeNbZ0U548ajrcTERlR+g1+M0sGvgNcC5QBt5hZWY9uDwLr3X0e8HHg4bN47qDbX9cMoOAXEelFJCP+RcBOd9/t7u3AM8ANPfqUAS8DuPt2oNTMCiN87qCrPBH8muoREekpkuAvAvaHPa4M2sJtAG4EMLNFwBSgOMLnEjxviZlVmFlFbW1tZNX3obKuBYASBb+IyGkiCf7eLl3lPR4/BOSb2XrgPmAd0Bnhc0ON7svcvdzdywsKCiIoq2/7jzSTk55C7qiUAb2OiEg8iiQZK4GSsMfFwIHwDu7eANwBYKFrHO4Jbpn9PXcoVNa1UDwmU5dbFBHpRSQj/tXATDObamZpwM3A8+EdzCwvWAdwN/Bq8Mug3+cOhcq6Fu3YFRHpQ78jfnfvNLN7gRVAMvCYu28xs3uC9UuB2cCTZtYFbAXuOtNzh+ajnKiX/XXNXD5j7FC+jYjIiBXRJLi7LweW92hbGrb8BjAz0ucOpbrmDprbu7RjV0SkD3H3zd1KHcMvInJGcRj8oUM5dQy/iEjv4jD4QyP+Io34RUR6FXfBX1XXQk5GCqNHpUa7FBGRmBR3wV9Z10JRnkb7IiJ9ibvgrzqqY/hFRM4k/oJfI34RkTOKq+Cvb+kIzsOvI3pERPoSV8FfFRzKqSN6RET6FlfBf+JQTk31iIj0Ka6Cv+qoRvwiIv2Jr+CvayEjNYmxWWn9dxYRSVDxFfxHQ0f06Dz8IiJ9i7/g1xE9IiJnFFfBr2/tioj0L26Cv6vbee95BSyamh/tUkREYlrcXI08Ocn4xkcXRLsMEZGYFzcjfhERiYyCX0QkwSj4RUQSjIJfRCTBKPhFRBKMgl9EJMEo+EVEEoyCX0QkwZi7R7uG05hZLfBOtOvowzjgcLSLOAPVNzCqb2BU38AMpL4p7l4QSceYDP5YZmYV7l4e7Tr6ovoGRvUNjOobmOGqT1M9IiIJRsEvIpJgFPxnb1m0C+iH6hsY1Tcwqm9ghqU+zfGLiCQYjfhFRBKMgl9EJNG4e0LegL3AJmA9UBG0jQFeAt4O7vPD+n8B2AnsAD4Q1n5R8Do7gW9zcvosHfhJ0P4mUNpPPY8BNcDmsLZhqQe4PXiPt4Hbz6K+LwFVwTZcD1wXxfpKgN8B24AtwGdiaRueob6Y2IZABrAK2BDU9+UY23591RcT2y/okwysA34dS9uu11rPNjDj5UYo+Mf1aPsa8ECw/ADw78FyWfAfLh2YCuwCkoN1q4DLAAN+A1wbtH8KWBos3wz8pJ96rgQWcmqwDnk9wX/O3cF9frCcH2F9XwI+30vfaNQ3EVgYLOcAbwV1xMQ2PEN9MbENg9fKDpZTCYXLpTG0/fqqLya2X9DvH4AfczL4Y2Lb9Zo3wxm2sXSj9+DfAUwM+0HdESx/AfhCWL8VwT/ORGB7WPstwCPhfYLlFELfxrN+airl1GAd8nrC+wTrHgFuibC+L9H7D11U6utRwy+Bq2NtG/ZSX8xtQyATWAtcEovbr0d9MbH9gGLgZeD9nAz+mNt2x2+JPMfvwEozW2NmS4K2Qnc/CBDcjw/ai4D9Yc+tDNqKguWe7ac8x907gXpg7FnWOBz19PVakbrXzDaa2WNmdvxK91Gtz8xKgQsJjQpjbhv2qA9iZBuaWbKZrSc0pfeSu8fU9uujPoiN7fct4H6gO6wtZrZdT4kc/Fe4+0LgWuDTZnblGfpaL21+hvYzPWcwDGY9A6nze8B0YAFwEPjPaNdnZtnAz4G/d/eGvvpFq8Ze6ouZbejuXe6+gNDodZGZze3tM8RYfVHffmZ2PVDj7mt66debqP/8Jmzwu/uB4L4G+AWwCKg2s4kAwX1N0L2S0M6544qBA0F7cS/tpzzHzFKA0cCRsyxzOOrp67X65e7VwQ9jN/D/CG3DqNVnZqmEQvUpd38uaI6ZbdhbfbG2DYOajgKvAIuJoe3XW30xsv2uAD5kZnuBZ4D3m9mPiMFtd0J/c0HxeAOygJyw5dcJ/Sf/OqfujPlasDyHU3fG7ObkzpjVhHYyHd8Zc13Q/mlO3Rnz0wjqKuXUOfQhr4fQTqE9hHYM5QfLYyKsb2LY8meBZ6JVX/B6TwLf6tEeE9vwDPXFxDYECoC8YHkU8Afg+hjafn3VFxPbL6yG93Jyjj8mtl2vdQ5H0MbaDZgWbPgNhA4N+6egfSyhHTRvB/djwp7zT4T2vu8g2NMetJcDm4N1/8XJw68ygGcJHX61CpjWT01PE/pTtYPQb/G7hqse4M6gfSdwx1nU90NCh55tBJ7n1B/C4a7vPYT+xN1I2KF9sbINz1BfTGxDYB6hQxE3Bq/9xeH8mRhAfTGx/cL6vZeTwR8T2663m07ZICKSYBJ2jl9EJFEp+EVEEoyCX0QkwSj4RUQSjIJfRCTBKPhFRBKMgl9EJMH8f+fdpkkLEZUsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "percent_of_all_weight = []\n",
    "s = cooc.sum()\n",
    "m = cooc.tocsc()\n",
    "for i in range(0, 400000, 2000):\n",
    "    p = m[:i,:i].sum() / s\n",
    "    percent_of_all_weight.append(p)\n",
    "del m\n",
    "pd.Series(percent_of_all_weight, index=list(range(0, 100000, 2000))+list(range(100000, 450000, 50000))).iloc[10:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Python GloVe\n",
    " - this doesn't do fine-tuning, but that's fine for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 cox\n",
      "4000 clothes\n",
      "6000 unexpected\n",
      "8000 favors\n",
      "10000 persecution\n",
      "12000 inspect\n",
      "14000 duchy\n",
      "16000 fairway\n",
      "18000 freddy\n",
      "20000 baths\n",
      "22000 hectic\n",
      "24000 long-distance\n",
      "26000 bose\n",
      "28000 steeply\n",
      "30000 mortally\n",
      "32000 juninho\n",
      "34000 elsie\n",
      "36000 707\n",
      "38000 unwind\n",
      "40000 1667\n",
      "42000 kirwan\n",
      "44000 banu\n",
      "46000 renton\n",
      "48000 utilise\n",
      "50000 bec\n",
      "52000 overpopulation\n",
      "54000 adhesives\n",
      "56000 masato\n",
      "58000 bolero\n",
      "60000 baek\n",
      "62000 scuppered\n",
      "64000 47-yard\n",
      "66000 towners\n",
      "68000 metros\n",
      "70000 b/w\n",
      "72000 akiba\n",
      "74000 boonyaratkalin\n",
      "76000 jeev\n",
      "78000 atiya\n",
      "80000 klinghoffer\n",
      "82000 hepworth\n",
      "84000 lovie\n",
      "86000 highrise\n",
      "88000 43d\n",
      "90000 azarov\n",
      "92000 liberté\n",
      "94000 75.2\n",
      "96000 rawdon\n",
      "98000 bushell\n",
      "100000 capron\n",
      "102000 940,000\n",
      "104000 laan\n",
      "106000 biwa\n",
      "108000 olisadebe\n",
      "110000 perpetua\n",
      "112000 douthit\n",
      "114000 11,900\n",
      "116000 hermaphrodites\n",
      "118000 antonucci\n",
      "120000 biratnagar\n",
      "122000 vrindavan\n",
      "124000 canvey\n",
      "126000 forêt\n",
      "128000 manston\n",
      "130000 12.74\n",
      "132000 arsdale\n",
      "134000 hauteur\n",
      "136000 unambitious\n",
      "138000 beckloff\n",
      "140000 yaffa\n",
      "142000 gimpel\n",
      "144000 caltanissetta\n",
      "146000 wingspans\n",
      "148000 skolnik\n",
      "150000 cryogenics\n",
      "152000 rekenthaler\n",
      "154000 ugk\n",
      "156000 kimco\n",
      "158000 83.50\n",
      "160000 ef1\n",
      "162000 us2\n",
      "164000 lajitas\n",
      "166000 so4\n",
      "168000 girlz\n",
      "170000 franchetti\n",
      "172000 baldivieso\n",
      "174000 take-offs\n",
      "176000 soirée\n",
      "178000 æthelfrith\n",
      "180000 blintzes\n",
      "182000 lege\n",
      "184000 moscow-based\n",
      "186000 nummelin\n",
      "188000 rampantly\n",
      "190000 birthstones\n",
      "192000 lotos\n",
      "194000 arnstadt\n",
      "196000 bonum\n",
      "198000 maurren\n",
      "200000 naadam\n",
      "202000 finalises\n",
      "204000 prairial\n",
      "206000 cryptococcus\n",
      "208000 farallones\n",
      "210000 concertation\n",
      "212000 chegutu\n",
      "214000 dindo\n",
      "216000 hourman\n",
      "218000 afrikan\n",
      "220000 lesticus\n",
      "222000 claddagh\n",
      "224000 simulants\n",
      "226000 jodhi\n",
      "228000 elitzur\n",
      "230000 containerboard\n",
      "232000 rén\n",
      "234000 zuffenhausen\n",
      "236000 obeidallah\n",
      "238000 proops\n",
      "240000 boydston\n",
      "242000 bidzina\n",
      "244000 62-20\n",
      "246000 storen\n",
      "248000 97.16\n",
      "250000 afterellen.com\n",
      "252000 esa-pekka\n",
      "254000 1.0-percent\n",
      "256000 sabesp\n",
      "258000 chedli\n",
      "260000 acuff-rose\n",
      "262000 171,500\n",
      "264000 hatsumi\n",
      "266000 http://www.uaw.org\n",
      "268000 xda\n",
      "270000 close-fitting\n",
      "272000 claudication\n",
      "274000 contendere\n",
      "276000 hoedspruit\n",
      "278000 shiliang\n",
      "280000 packbot\n",
      "282000 åsmund\n",
      "284000 kjellman\n",
      "286000 seguenzia\n",
      "288000 medicean\n",
      "290000 comptel\n",
      "292000 anahit\n",
      "294000 www.aol.com\n",
      "296000 1961-1965\n",
      "298000 veut\n",
      "300000 tanke\n",
      "302000 kanyakubja\n",
      "304000 boake\n",
      "306000 poundings\n",
      "308000 tapiwa\n",
      "310000 saraju\n",
      "312000 ebags\n",
      "314000 portschach\n",
      "316000 zuiverloon\n",
      "318000 jeriome\n",
      "320000 rouiba\n",
      "322000 bissix\n",
      "324000 lepeltier\n",
      "326000 win.ini\n",
      "328000 hausch\n",
      "330000 discomfit\n",
      "332000 cadd\n",
      "334000 barrenechea\n",
      "336000 orlock\n",
      "338000 43-27\n",
      "340000 numurkah\n",
      "342000 torroella\n",
      "344000 us-only\n",
      "346000 civetta\n",
      "348000 apriantono\n",
      "350000 hla-a\n",
      "352000 mungret\n",
      "354000 williamsf1\n",
      "356000 stellone\n",
      "358000 plenderleith\n",
      "360000 90125\n",
      "362000 linesmen_lonnie\n",
      "364000 gunvessels\n",
      "366000 weyrs\n",
      "368000 non-korean\n",
      "370000 zipkin\n",
      "372000 stauss\n",
      "374000 104.97\n",
      "376000 huggin\n",
      "378000 terregles\n",
      "380000 lombarde\n",
      "382000 361.4\n",
      "384000 bird-man\n",
      "386000 martand\n",
      "388000 blewbury\n",
      "390000 1.137\n",
      "392000 sammes\n",
      "394000 siwoff\n",
      "396000 jingming\n",
      "398000 geoscheme\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000, 400000, 2000):\n",
    "    print(i, model_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264810141.26174298 0.05 40997530242.7755\n",
      "Performing 100 training epochs with 25 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n"
     ]
    }
   ],
   "source": [
    "n = 200000\n",
    "#fpath = '/data/motes/coocs/enwiki_full.npz'\n",
    "for fpath in ['/data/motes/coocs/all_weighted_10-12.npz']:\n",
    "    just_name = os.path.splitext(os.path.split(fpath)[-1])[0]\n",
    "\n",
    "    cooc = sparse.load_npz(fpath)\n",
    "    # Load Sparse Matrix\n",
    "    print(cooc.data.max(), cooc.data.min(), cooc.data.sum())\n",
    "\n",
    "    # Truncate just to top n words in vocab\n",
    "    if n < 400000:\n",
    "        m = cooc.tocsc()[:n, :n].tocoo()\n",
    "    else:\n",
    "        m = cooc.tocoo()\n",
    "\n",
    "    glove = Glove(no_components=300, learning_rate=0.05) \n",
    "    glove.fit(m, epochs=100, no_threads=25, verbose=True)\n",
    "    del m\n",
    "    dictionary = dict(list(model_dict.token2id.items())[:n])\n",
    "    glove.add_dictionary(dictionary)\n",
    "\n",
    "    # Convert to keyed vectors and save\n",
    "    kv = modeling.glove_to_keyedvectors(glove)\n",
    "    kv.save('/data/motes/models/{}_{}k.kv'.format(just_name, int(n/1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10650"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000\n",
    "word_sums = pd.Series(np.array(full_mat.sum(1))[:n, 0], index=glove.dictionary.keys())\n",
    "\n",
    "# only preserve vocab where there have been enough words seen\n",
    "with open('/data/motes/all_weighted_1_model_include_vocab.txt', mode='w') as f:\n",
    "    f.write('\\n'.join(word_sums[word_sums > 500].index.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
